{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yeonheuiyeon/Machine-Learning-Practice/blob/master/06_02_20181237%EC%97%B0%ED%9D%AC%EC%97%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1-dJn-pMMNz9"
   },
   "source": [
    ">## 수고하셨습니다! 종강까지 얼마 안남았네요. 화이팅하세요!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YjGC6K7zW4PX"
   },
   "source": [
    "# 학습보고서\n",
    "\n",
    "우리가 \"이미지 변환 네트\"라고 부르는 우리의 generator 를 통과하고 나면,  이런 종류의 U-Net 형상을 가진 것을 볼 수 있다.실제로 U-Net를 사용하지 않았다. 왜냐하면 이것이 나왔을 때, 기계 학습 세계에서는 아무도 U-Net에 대해 알지 못했기 때문이다. 물론 요즘은 유넷을 사용한다. \n",
    "\n",
    "다운샘플링 경로 다음에 업샘플링 경로가 있는 이런 종류의 아키텍처에서는 다운샘플링 경로를 우리 코드에서 보셨듯이 인코더라고 부름. 그리고 업샘플링 경로는 디코더라고 불린다. 생성 모델에서는 일반적으로 text models, neural translation, stuff 등을 포함하며 인코더와 디코더라고 불리는 경향이 있다.\n",
    "\n",
    "어쨌든, 우리는 이 generator를 가지고 있고 \"우리가 원하는 것과 같은 것이 만들어졌는가?\"라고 하는 loss function 를 원한다. 그래서 그들이 하는 방법은 예측을 취하는 것이다. 기억하라, \\hat{y}는 우리가 보통 모델의 예측을 위해 사용하는 것이다. 예측을 통해 미리 훈련된 ImageNet 네트워크를 통해 예측을 한다. 이 사실이 알려졌을 당시 이들이 사용하던 사전 훈련된 ImageNet 네트워크는 VGG였다. 지금은 좀 낡았지만, 이런 과정에서는 잘 작동하기 때문에 사람들은 여전히 그것을 사용한다고 한다. 그래서 그들은 예측을 하고 미리 훈련된 ImageNet 네트워크인 VGG를 통해 예측을 한다. \n",
    "\n",
    "그래서 보통은, \"이봐, 이게 강아지, 고양이, 비행기, 소방차, 뭐 그런것들이야?\"라고 말할 겁니다. 하지만 최종 분류에 도달하는 과정에서, 그것은 많은 다른 층들을 거친다. 이 경우 그리드의 크기가 같은 모든 레이어와 색상의 피쳐 맵을 컬러 코딩한 것이다. 그래서 우리가 색을 바꿀 때마다, 우리는 격자 크기를 바꾼다. 그래서 2개의 콘벤트나 VGG의 경우 그들은 여전히 비슷한 생각인 몇 개의 막스풀링 레이어를 사용함.\n",
    "\n",
    "우리가 할 수 있는 것은 이 생성된 이미지에 대한 VGG 모델의 최종 결과물을 가져가지 말고 중간에서 무언가를 가져보자. 중간에 있는 어떤 층의 활성화를 취한다. 이러한 활성화는 약 256개의 채널이 28x28로 표시되는 형상도일 수 있다. 그래서 28x28 격자 세포들은 대략 의미론적으로 \"28x28 격자 이 부분에, 모피처럼 보이는 어떤 것이 있는가? 아니면 반짝반짝 빛나는 것이 있을까? 아니면 뭔가 순환적인 것이 있을까? 그런 눈알 같은 게 있을까?\"\n",
    "\n",
    "그래서 우리가 하는 일은 목표물(즉, 실제 y 값)을 가져다가 미리 훈련된 동일한 VGG 네트워크를 통해 같은 계층의 활성화를 끌어내는 겁니다. 그리고 나서 우리는 평균 제곱 오차를 비교한다. 그래서 \"실제 이미지에서 28x28 피쳐 맵의 그리드 셀 (1, 1)은 털이 많고 파란색이며 둥근 모양을 하고 있다. 그리고 생성된 이미지에서는 털북숭이하고 파랗고 둥근 모양이 아니랍니다.\" \n",
    "\n",
    "우리의 eyeball problem를 해결하는 데는 큰 도움이 될 것이다. 왜냐하면 이 경우, 피쳐 맵에는 \"여기 (대상물에) 안구가 있지만, 여기 (생성된 버전에)  eyeball가 없으니, 좀 더 잘 해달라\"고 적혀 있을 것이기 때문이다. 더 좋은 눈알을 만들어라.\" 그래서 그런 생각이야. 그것이 우리가 특징적 손실이라고 부르는 것이고 존슨 외에서는 지각적 손실이라고 부르는 것이다.\n",
    "JPEG 품질은 항상 60이고, W 텍스트는 없다. 꼭대기에서 96 X 96을 찍으면 96 X 96이야. 그리고 \"crappify\"가 얼마나 대단한 단어인지 깨닫기 resize_one이라고 한다.\n",
    "\n",
    "형편없는 이미지와 좋은 이미지 두 개 가져옴. 나는 이것을 하는 (지각적 손실) 기능을 만들려고 한다. 내가 가장 먼저 하는 일은 \"픽셀과 형상을 어떻게 비교할 것인가?\"와 같은 기본 loss function를 정의해야함.그리고 선택은 주로 MSE나 L1과 같다. 어느 쪽을 선택하든 크게 상관없다. \n",
    "\n",
    "사전 훈련된 모델을 사용해서 VGG 모델을 만든다. VGG에는 모델의 콘볼루션 부분을 포함하는 .features라는 속성이 있다. 그래서 vg16_bn(True).형상은 VGG 모델의 경련성 부분이다. Head가 필요 없기 때문이다. 중간활동만 사용한다.\n",
    "\n",
    "GPU에서 평가 모드로 확인한다. 그리고 우리는 이 모델의 무게를 업데이트하고 싶지 않기 때문에 need_grad를 끌 것이다. 우리는 단지 추론(즉, 손실)을 위해 그것을 사용하고 있을 뿐이다.\n",
    "\n",
    "그러면 해당 모델의 모든 하위 항목을 열거하고 모든 max pooling layers를 찾아봅시다. VGG 모델에서는 그리드 크기가 변경되기 때문이다. 이 사진에서 볼 수 있듯이 그리드 크기가 변경되기 바로 전에 모든 기능을 활용해야된다.\n",
    "\n",
    "그럼 1층을 잡자 그것이 변하기 전의 층이다. 따라서 max pooling layers바로 앞에 레이어 번호 목록이 있다([5, 12, 22, 32, 42]). 그것을 블록에 넣는다. 단지 ID의 목록일 뿐이다.\n",
    "\n",
    "기본적으로 feature loss class라고 부를 때, 우리는 그것을 m_feat라고 불릴 미리 훈련된 어떤 모델을 통과시킬 것이다. 그것은 우리가 기능 상실을 원하는 특징을 포함하고 있는 모델이다. 그래서 우리는 그 네트워크로부터 모든 계층을 얻을 수 있는데, 그 계층들은 우리가 원하는 기능들이 손실을 만들어 낼 수 있다.\n",
    "\n",
    "우리는 이 모든 출력물들을 연결해야 한다.  PyTorch의 중간 레이어를 잡는 방법은 고리를 연결시키는 겁니다.  self.hook 는 우리의 후크 출력을 포함한다.\n",
    "\n",
    "loss의 전 단계에서는 목표물을 통과하는 make_features(즉, 우리의 실제 y)를 호출하여 VGG 모델을 호출하고 저장된 모든 활성화 과정을 거친 후 복사본을 가져온다. 대상(out_feat)과 입력(in_feat) 모두에 대해 이 작업을 수행할 것이며, 이는 제너레이터(in_feat)의 출력이다. 이제 픽셀 사이의 L1 손실을 계산진행 후 여전히 픽셀을 약간 잃기를 원하기 때문이다. 그럼 그 층들의 특징들을 살펴서 L1 손실을 입히도록 합시다. 그래서 우리는 기본적으로 각 블록의 이 모든 끝을 살펴보고, 각 블록의 활성화와 L1을 각각 얻는다.\n",
    "\n",
    "그렇게 되면 feat_losses라는 이 리스트에 들어가게 되고, 그 리스트를 모두 합쳐서 정리하게 될 것이다. list로 하는이유는 우리가 이 멋진 콜백을 가지고 있기 때문에, 만약 당신이 그것들을 분실기능에 있는 .metrics라고 불리는 것에 넣으면, 그것은 당신에게 매우 유용한 별개의 층 손실량을 모두 출력해 줄 것이다.\n",
    "\n",
    "이제 우리는 우리의 데이터와 미리 훈련된 아키텍처인 ResNet 34로 U-Net을 통상적인 방법으로 훈련시킬 수 있으며, 사전 훈련된 VGG 모델을 사용하는 손실 기능을 사용할 수 있다. 이 (콜백_fns)는 내가 LossMetrics에 대해 언급한 콜백으로, 우리를 위해 모든 층의 손실을 출력할 것이다. 이 두 가지 사항(블러와 norm_type)은 본 코스의 2부에서 배우겠지만, 반드시 사용해야 한다.\n",
    "\n",
    "평소처럼 유넷에서 사전 훈련된 네트워크를 사용하고 있기 때문에 다운샘플링 경로에 대해서는 냉동 레이어부터 시작해 잠시 훈련한다. 보다시피, 우리는 손실뿐만 아니라 픽셀 손실과 각 피쳐 레이어에서의 손실도 얻는다. 그리고 그램 손실이라고 불리는 파트 2에서 우리가 알게 될 것이다. 내가 알기로는 누구도 슈퍼 해상도를 위해 사용되지는 않았을 것이다. \n",
    "\n",
    "이건 8분, GAN보다 훨씬 빠른 속도이다. 그리고 나서 우리는 freeze을 풀고 조금 더 훈련한다. 그리고 사이즈를 두 배로 바꾼다. 그 후 GPU 메모리가 소진되지 않도록 배치 크기를 절반으로 줄여야 하고, 다시 동결하고, 좀 더 훈련시켜야 한다.\n",
    "\n",
    "JPEG 공예품들이 귀를 감싸고 있는 것에서부터 시작되었고, 이 모든 혼란과 눈들이 일종의 모호한 연한 푸른 색의 것들로 시작되었고, 그것은 정말로 많은 질감을 만들어냈다. 이 고양이는 솜털로 뒤덮인 작은 발톱틀의 윗부분을 들여다보고 있는 것이 분명해. 그래서 실제로 이 것이 아마도 카펫 재질일 것이라는 것을 인식했지. 그것은 우리를 위해 카펫 재료를 만들었다.\n",
    "\n",
    "## Medium Resolution \n",
    "이제 우리가 할 수 있는 것은, 우리의 낮은 res로 시작하는 대신에, 나는 실제로 중간 res라고 불리는 256 사이즈로 다른 세트를 저장해 놓았으니, 만약 우리가 미디엄res 사이즈를 올리면 어떻게 되는지 봅시다.\n",
    "우리는 미디엄 res 데이터를 수집할 것이고, 여기 미디엄 res 저장 사진 입니다. 우리가 이것을 개선할 수 있을까? 보다시피, 아직 개선의 여지가 많다. 여기 속눈썹이 굉장히 픽셀화 되어 있게 결과가 도출된다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FbWcL_ZIW9Ge"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tH5PZjOZdewm"
   },
   "source": [
    "파이참 디버깅 사진\n",
    "https://drive.google.com/drive/folders/1q61FjRhwa7cVF2kqatB-xvGVDQfQkp3a?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z-ziL52Idhpv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPuWQdjj+9m44OW2g++gwA8",
   "include_colab_link": true,
   "name": "06.02_20181237연희연.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
